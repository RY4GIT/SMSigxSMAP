{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize and Access NSIDC DAAC Data\n",
    "\n",
    "This notebook will walk you through how to programmatically access data from the NASA National Snow and Ice Data Center Distributed Active Archive Center (NSIDC DAAC) using spatial and temporal filters, as well as how to request customization services including subsetting, reformatting, and reprojection. No Python experience is necessary; each code cell will prompt you with the information needed to configure your data request. The notebook will print the resulting API command that can be used in a command line, browser, or in Python as executed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import getpass\n",
    "import socket \n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import pprint\n",
    "import re\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import matplotlib.pyplot as plt\n",
    "# To read KML files with geopandas, we will need to enable KML support in fiona (disabled by default)\n",
    "fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Earthdata Login credentials\n",
    "\n",
    "An Earthdata Login account is required to access data from the NSIDC DAAC. If you do not already have an Earthdata Login account, visit http://urs.earthdata.nasa.gov to register."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_credential_path = \"../auth.json\"\n",
    "with open(my_credential_path, 'r') as infile:\n",
    "    my_credentials = json.load(infile)\n",
    "    \n",
    "uid = my_credentials['username'] # Enter Earthdata Login user name\n",
    "pswd = my_credentials['password'] # Enter Earthdata Login password\n",
    "email = my_credentials['email'] # Enter Earthdata login email \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imput data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most recent version of  MOD15A2H  is  061\n"
     ]
    }
   ],
   "source": [
    "# Get json response from CMR collection metadata\n",
    "short_name = 'MOD15A2H'\n",
    "params = {\n",
    "    'short_name': short_name\n",
    "}\n",
    "\n",
    "cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "response = requests.get(cmr_collections_url, params=params)\n",
    "results = json.loads(response.content)\n",
    "\n",
    "# Find all instances of 'version_id' in metadata and print most recent version number\n",
    "\n",
    "versions = [el['version_id'] for el in results['feed']['entry']]\n",
    "latest_version = max(versions)\n",
    "print('The most recent version of ', short_name, ' is ', latest_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select time period of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input temporal range \n",
    "\n",
    "start_date = '2015-03-31'# input('Input start date in yyyy-MM-dd format: ')\n",
    "start_time = '00:00:00' # input('Input start time in HH:mm:ss format: ')\n",
    "end_date = '2022-03-30' # input('Input end date in yyyy-MM-dd format: ')\n",
    "end_time = '00:00:00' # input('Input end time in HH:mm:ss format: ')\n",
    "\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select area of interest\n",
    "\n",
    "#### Select bounding box or shapefile entry\n",
    "\n",
    "For all data sets, you can enter a bounding box to be applied to your file search. If you are interested in ICESat-2 data, you may also apply a spatial boundary based on a vector-based spatial data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter spatial coordinates in decimal degrees, with west longitude and south latitude reported as negative degrees. Do not include spaces between coordinates.\n",
    "# Example over the state of Colorado: -109,37,-102,41\n",
    "bounding_box = '147.534,-35.324,147.535,-35.323' #input('Input spatial coordinates in the following order: lower left longitude,lower left latitude,upper right longitude,upper right latitude. Leave blank if you wish to provide a vector-based spatial file for ICESat-2 search and subsetting:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine how many granules exist over this time and area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 322 granules of MOD15A2H version 061 over my area and time of interest.\n"
     ]
    }
   ],
   "source": [
    "# Create CMR parameters used for granule search. Modify params depending on bounding_box or polygon input.\n",
    "\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "aoi='1'\n",
    "if aoi == '1':\n",
    "# bounding box input:\n",
    "    search_params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'bounding_box': bounding_box\n",
    "    }\n",
    "\n",
    "granules = []\n",
    "headers={'Accept': 'application/json'}\n",
    "while True:\n",
    "    response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    search_params['page_num'] += 1\n",
    "\n",
    "print('There are', len(granules), 'granules of', short_name, 'version', latest_version, 'over my area and time of interest.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the average size of those granules as well as the total volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average size of each granule is 4.47 MB and the total size of all 322 granules is 1438.54 MB\n"
     ]
    }
   ],
   "source": [
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "granule_urls = [granule['links'][0]['href'] for granule in granules]\n",
    "print(f'The average size of each granule is {mean(granule_sizes):.2f} MB and the total size of all {len(granules)} granules is {sum(granule_sizes):.2f} MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOD15A2H.A2015089.h30v12.061.2021322004847.hdf 4694515\n",
      "MOD15A2H.A2015097.h30v12.061.2021323034548.hdf 4595580\n",
      "MOD15A2H.A2015105.h30v12.061.2021323062129.hdf 4743115\n",
      "MOD15A2H.A2015113.h30v12.061.2021323182029.hdf 4927470\n",
      "MOD15A2H.A2015121.h30v12.061.2021324003130.hdf 4537196\n",
      "MOD15A2H.A2015129.h30v12.061.2021326041248.hdf 4706099\n",
      "MOD15A2H.A2015137.h30v12.061.2021326062224.hdf 4863406\n",
      "MOD15A2H.A2015145.h30v12.061.2021326082512.hdf 4484858\n",
      "MOD15A2H.A2015153.h30v12.061.2021326112629.hdf 4684239\n",
      "MOD15A2H.A2015161.h30v12.061.2021326135531.hdf 4905224\n",
      "MOD15A2H.A2015169.h30v12.061.2021326171123.hdf 5167728\n",
      "MOD15A2H.A2015177.h30v12.061.2021326233525.hdf 4908146\n",
      "MOD15A2H.A2015185.h30v12.061.2021328001303.hdf 4983539\n",
      "MOD15A2H.A2015193.h30v12.061.2021328054910.hdf 5191019\n",
      "MOD15A2H.A2015201.h30v12.061.2021329051619.hdf 5092892\n",
      "MOD15A2H.A2015209.h30v12.061.2021329150918.hdf 5172800\n",
      "MOD15A2H.A2015217.h30v12.061.2021330000513.hdf 5306172\n",
      "MOD15A2H.A2015225.h30v12.061.2021331171425.hdf 5308971\n",
      "MOD15A2H.A2015233.h30v12.061.2021331200951.hdf 5529327\n",
      "MOD15A2H.A2015241.h30v12.061.2021331221056.hdf 5416205\n",
      "MOD15A2H.A2015249.h30v12.061.2021332000053.hdf 5220470\n",
      "MOD15A2H.A2015257.h30v12.061.2021332105652.hdf 5136378\n",
      "MOD15A2H.A2015265.h30v12.061.2021332173811.hdf 5107577\n",
      "MOD15A2H.A2015273.h30v12.061.2021333064917.hdf 4694785\n",
      "MOD15A2H.A2015281.h30v12.061.2021333150211.hdf 4780589\n",
      "MOD15A2H.A2015289.h30v12.061.2021335160333.hdf 4712218\n",
      "MOD15A2H.A2015297.h30v12.061.2021335211559.hdf 4550280\n",
      "MOD15A2H.A2015305.h30v12.061.2021336000936.hdf 5076626\n",
      "MOD15A2H.A2015313.h30v12.061.2021336163111.hdf 4615924\n",
      "MOD15A2H.A2015321.h30v12.061.2021337031127.hdf 4324741\n",
      "MOD15A2H.A2015329.h30v12.061.2021337105958.hdf 4327932\n",
      "MOD15A2H.A2015337.h30v12.061.2021342140514.hdf 4287532\n",
      "MOD15A2H.A2015345.h30v12.061.2021342170848.hdf 4339992\n",
      "MOD15A2H.A2015353.h30v12.061.2021342200100.hdf 4456125\n",
      "MOD15A2H.A2015361.h30v12.061.2021342225454.hdf 4320847\n",
      "MOD15A2H.A2016001.h30v12.061.2021343012417.hdf 4547754\n",
      "MOD15A2H.A2016009.h30v12.061.2021343040940.hdf 4357975\n",
      "MOD15A2H.A2016017.h30v12.061.2021343074918.hdf 4791377\n",
      "MOD15A2H.A2016025.h30v12.061.2021343142446.hdf 4861044\n",
      "MOD15A2H.A2016033.h30v12.061.2021346043634.hdf 4702207\n",
      "MOD15A2H.A2016041.h30v12.061.2021346073818.hdf 4508805\n",
      "MOD15A2H.A2016057.h30v12.061.2021346114204.hdf 4338821\n",
      "MOD15A2H.A2016065.h30v12.061.2021346144810.hdf 4362008\n",
      "MOD15A2H.A2016073.h30v12.061.2021346171141.hdf 4440647\n",
      "MOD15A2H.A2016081.h30v12.061.2021346205926.hdf 4318164\n",
      "MOD15A2H.A2016089.h30v12.061.2021347064129.hdf 4202253\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "def save_data(data, filename,destination_folder,overwrite=False):\n",
    "    '''\n",
    "    Save data in data to destination_folder/filename\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "\n",
    "    data   : binary array\n",
    "    filename : string: name of output file\n",
    "    destination_folder: name of output directory (created if not existing)\n",
    "\n",
    "    Keyword Arguments:\n",
    "    -----------------\n",
    "\n",
    "    overwrite : overwrite file if it already exists\n",
    "    '''\n",
    "    dest_path = Path(destination_folder)\n",
    "    if not dest_path.exists():\n",
    "        dest_path.mkdir()\n",
    "\n",
    "    output_fname = dest_path.joinpath(filename)\n",
    "\n",
    "    d = 0\n",
    "    if overwrite or (not output_fname.exists()):  \n",
    "        with open(output_fname, 'wb') as fp:\n",
    "            d = fp.write(data)\n",
    "\n",
    "    return(d)\n",
    "\n",
    "saveDir= r'G:\\Shared drives\\Ryoko and Hilary\\SMSigxSMAP\\analysis\\1_data\\SMAP\\OZNET\\MODIS-LAI'\n",
    "# loop over urls\n",
    "for url in granule_urls:\n",
    "    r = session.request('get',url)\n",
    "\n",
    "    # check response\n",
    "    if r.ok:\n",
    "        # get the filename from the url\n",
    "        filename = url.split('/')[-1]\n",
    "        # get the binary data\n",
    "        d = save_data(r.content, filename, saveDir)\n",
    "        print(filename, d)\n",
    "    else:\n",
    "        print (f'response from {url} not good')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we will clean up the Output folder by removing individual order folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Outputs folder by removing individual granule folders \n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            shutil.move(os.path.join(root, file), path)\n",
    "        except OSError:\n",
    "            pass\n",
    "    for name in dirs:\n",
    "        os.rmdir(os.path.join(root, name))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To review, we have explored data availability and volume over a region and time of interest, discovered and selected data customization options, constructed an API endpoint for our request, and downloaded data directly to our local machine. You are welcome to request different data sets, areas of interest, and/or customization services by re-running the notebook or starting again at the 'Select a data set of interest' step above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('SMAP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6fd90ae4aea728ba3e3561c83716ba8d838e0a31868a80e606679a5b06aceed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
