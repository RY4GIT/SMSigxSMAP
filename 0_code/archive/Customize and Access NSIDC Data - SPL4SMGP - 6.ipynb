{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize and Access NSIDC DAAC Data\n",
    "\n",
    "This notebook will walk you through how to programmatically access data from the NASA National Snow and Ice Data Center Distributed Active Archive Center (NSIDC DAAC) using spatial and temporal filters, as well as how to request customization services including subsetting, reformatting, and reprojection. No Python experience is necessary; each code cell will prompt you with the information needed to configure your data request. The notebook will print the resulting API command that can be used in a command line, browser, or in Python as executed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import pprint\n",
    "import re\n",
    "import time\n",
    "from statistics import mean\n",
    "%matplotlib inline\n",
    "\n",
    "short_name = 'SPL4SMGP'\n",
    "out_path = r'..\\1_data'\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "   os.makedirs(out_path)\n",
    "\n",
    "if not os.path.exists(os.path.join(out_path, short_name)):\n",
    "   os.makedirs(os.path.join(out_path, short_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Earthdata Login credentials\n",
    "\n",
    "An Earthdata Login account is required to access data from the NSIDC DAAC. If you do not already have an Earthdata Login account, visit http://urs.earthdata.nasa.gov to register."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_credential_path = \"./auth.json\"\n",
    "with open(my_credential_path, 'r') as infile:\n",
    "    my_credentials = json.load(infile)\n",
    "    \n",
    "uid = my_credentials['username'] # Enter Earthdata Login user name\n",
    "pswd = my_credentials['password'] # Enter Earthdata Login password\n",
    "email = my_credentials['email'] # Enter Earthdata login email "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data set and determine version number\n",
    "\n",
    "Data sets are selected by data set IDs (e.g. MOD10A1), whic are also referred to as a \"short name\". These short names are located at the top of each NSIDC data set landing page in gray above the full title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most recent version of  SPL4SMGP  is  007\n"
     ]
    }
   ],
   "source": [
    "# Get json response from CMR collection metadata\n",
    "\n",
    "params = {\n",
    "    'short_name': short_name\n",
    "}\n",
    "\n",
    "cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "response = requests.get(cmr_collections_url, params=params)\n",
    "results = json.loads(response.content)\n",
    "\n",
    "# Find all instances of 'version_id' in metadata and print most recent version number\n",
    "\n",
    "versions = [el['version_id'] for el in results['feed']['entry']]\n",
    "latest_version = max(versions)\n",
    "print('The most recent version of ', short_name, ' is ', latest_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select time period of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input temporal range \n",
    "\n",
    "# start_date = '2015-03-31'# input('Input start date in yyyy-MM-dd format: ')\n",
    "# start_date = '2015-12-05'# input('Input start date in yyyy-MM-dd format: ')\n",
    "# start_date = '2016-05-25'# input('Input start date in yyyy-MM-dd format: ')\n",
    "# 2021-10-01\n",
    "start_date = '2018-07-01'# input('Input start date in yyyy-MM-dd format: ')\n",
    "start_time = '00:00:00' # input('Input start time in HH:mm:ss format: ')\n",
    "end_date = '2019-01-01' # input('Input end date in yyyy-MM-dd format: ')\n",
    "end_time = '00:00:00' # input('Input end time in HH:mm:ss format: ')\n",
    "\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine how many granules exist over this time and area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1474 granules of SPL4SMGP version 007 over my area and time of interest.\n",
      "The average size of each granule is 143.54 MB and the total size of all 1474 granules is 211577.59 MB\n"
     ]
    }
   ],
   "source": [
    "# Create CMR parameters used for granule search. Modify params depending on bounding_box or polygon input.\n",
    "\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "aoi='1'\n",
    "if aoi == '1':\n",
    "# bounding box input:\n",
    "    search_params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    # 'bounding_box': bounding_box\n",
    "    }\n",
    "\n",
    "granules = []\n",
    "headers={'Accept': 'application/json'}\n",
    "while True:\n",
    "    response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    search_params['page_num'] += 1\n",
    "\n",
    "print('There are', len(granules), 'granules of', short_name, 'version', latest_version, 'over my area and time of interest.')\n",
    "\n",
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "print(f'The average size of each granule is {mean(granule_sizes):.2f} MB and the total size of all {len(granules)} granules is {sum(granule_sizes):.2f} MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that subsetting, reformatting, or reprojecting can alter the size of the granules if those services are applied to your request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the subsetting, reformatting, and reprojection services enabled for your data set of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NSIDC DAAC supports customization services on many of our NASA Earthdata mission collections. Let's discover whether or not our data set has these services available. If services are available, we will also determine the specific service options supported for this data set and select which of these services we want to request. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the service capability endpoint to gather service information needed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Query service capability URL \n",
    "from xml.etree import ElementTree as ET\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "\n",
    "# Create session to store cookie and pass credentials to capabilities url\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "response = session.get(s.url,auth=(uid, pswd))\n",
    "print(response)\n",
    "root = ET.fromstring(response.content)\n",
    "\n",
    "#collect lists with each service option\n",
    "subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "if len(subagent) > 0 :\n",
    "\n",
    "    # variable subsetting\n",
    "    variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "    variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "    variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "    variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "    # reformatting\n",
    "    formats = [Format.attrib for Format in root.iter('Format')]\n",
    "    format_vals = [formats[i]['value'] for i in range(len(formats))]\n",
    "    format_vals.remove('')\n",
    "\n",
    "    # reprojection options\n",
    "    projections = [Projection.attrib for Projection in root.iter('Projection')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subsetting, reformatting, and reprojection service options, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These reformatting options are available: ['HDF-EOS5', 'NetCDF4-CF', 'ASCII', 'KML', 'GeoTIFF']\n",
      "These reprojection options are available with your requested format: ['GEOGRAPHIC', 'UNIVERSAL TRANSVERSE MERCATOR', 'CYLINDRICAL EQUAL AREA', 'NORTH POLAR STEREOGRAPHIC', 'SOUTH POLAR STEREOGRAPHIC']\n"
     ]
    }
   ],
   "source": [
    "#print service information depending on service availability and select service options\n",
    "    \n",
    "if len(subagent) < 1 :\n",
    "    print('No services exist for', short_name, 'version', latest_version)\n",
    "    agent = 'NO'\n",
    "    bbox = ''\n",
    "    time_var = ''\n",
    "    reformat = ''\n",
    "    projection = ''\n",
    "    projection_parameters = ''\n",
    "    coverage = ''\n",
    "    Boundingshape = ''\n",
    "else:\n",
    "    # These reformatting options are available: ['HDF-EOS5', 'NetCDF4-CF', 'ASCII', 'KML', 'GeoTIFF']\n",
    "    # These reprojection options are available with your requested format: ['GEOGRAPHIC', 'UNIVERSAL TRANSVERSE MERCATOR', 'CYLINDRICAL EQUAL AREA', 'NORTH POLAR STEREOGRAPHIC', 'SOUTH POLAR STEREOGRAPHIC']\n",
    "    agent = ''\n",
    "    subdict = subagent[0]\n",
    "    if subdict['spatialSubsetting'] == 'true' and aoi == '1':\n",
    "        Boundingshape = ''\n",
    "        ss = 'n' # input('Subsetting by bounding box, based on the area of interest inputted above, is available. Would you like to request this service? (y/n)')\n",
    "        if ss == 'y': bbox = bounding_box\n",
    "        else: bbox = '' \n",
    "    if subdict['spatialSubsettingShapefile'] == 'true' and aoi == '2':\n",
    "        bbox = ''\n",
    "        ps = 'n' # input('Subsetting by geospatial file (Esri Shapefile, KML, etc.) is available. Would you like to request this service? (y/n)')\n",
    "        if ps == 'y': Boundingshape = geojson\n",
    "        else: Boundingshape = '' \n",
    "    if subdict['temporalSubsetting'] == 'true':\n",
    "        ts = 'y'\n",
    "        if ts == 'y': time_var = start_date + 'T' + start_time + ',' + end_date + 'T' + end_time \n",
    "        else: time_var = ''\n",
    "    else: time_var = ''\n",
    "    if len(format_vals) > 0 :\n",
    "        print('These reformatting options are available:', format_vals)\n",
    "        reformat = 'NetCDF4-CF'\n",
    "        if reformat == 'n': reformat = '' # Catch user input of 'n' instead of leaving blank\n",
    "    else: \n",
    "        reformat = ''\n",
    "        projection = ''\n",
    "        projection_parameters = ''\n",
    "    if len(projections) > 0:\n",
    "        valid_proj = [] # select reprojection options based on reformatting selection\n",
    "        for i in range(len(projections)):\n",
    "            if 'excludeFormat' in projections[i]:\n",
    "                exclformats_str = projections[i]['excludeFormat'] \n",
    "                exclformats_list = exclformats_str.split(',')\n",
    "            if ('excludeFormat' not in projections[i] or reformat not in exclformats_list) and projections[i]['value'] != 'NO_CHANGE': valid_proj.append(projections[i]['value'])\n",
    "        if len(valid_proj) > 0:\n",
    "            print('These reprojection options are available with your requested format:', valid_proj)\n",
    "            projection = 'GEOGRAPHIC' #input('If you would like to reproject, copy and paste the reprojection option you would like (make sure to omit quotes), otherwise leave blank.')\n",
    "            # Enter required parameters for UTM North and South\n",
    "            if projection == 'UTM NORTHERN HEMISPHERE' or projection == 'UTM SOUTHERN HEMISPHERE': \n",
    "                NZone = input('Please enter a UTM zone (1 to 60 for Northern Hemisphere; -60 to -1 for Southern Hemisphere):')\n",
    "                projection_parameters = str('NZone:' + NZone)\n",
    "            else: projection_parameters = ''\n",
    "        else: \n",
    "            print('No reprojection options are supported with your requested format')\n",
    "            projection = ''\n",
    "            projection_parameters = ''\n",
    "    else:\n",
    "        print('No reprojection options are supported with your requested format')\n",
    "        projection = ''\n",
    "        projection_parameters = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SPL4SMGP variables to select from include:\n"
     ]
    }
   ],
   "source": [
    "# # Select variable subsetting\n",
    "# print(*variable_vals, sep = \"\\n\") \n",
    "if len(subagent) > 0 :\n",
    "    if len(variable_vals) > 0:\n",
    "        v = 'y'\n",
    "        if v == 'y':\n",
    "            print('The', short_name, 'variables to select from include:')\n",
    "            \n",
    "            coverage = '/Geophysical_Data/precipitation_total_surface_flux,/cell_column,/cell_row,/cell_lat,/cell_lon'\n",
    "        else: coverage = ''\n",
    "\n",
    "#no services selected\n",
    "if reformat == '' and projection == '' and projection_parameters == '' and coverage == '' and time_var == '' and bbox == '' and Boundingshape == '':\n",
    "    agent = 'NO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data access configurations\n",
    "\n",
    "The data request can be accessed asynchronously or synchronously. The asynchronous option will allow concurrent requests to be queued and processed without the need for a continuous connection. Those requested orders will be delivered to the specified email address, or they can be accessed programmatically as shown below. Synchronous requests will automatically download the data as soon as processing is complete. The granule limits differ between these two options:\n",
    "\n",
    "Maximum granules per synchronous request = 100 \n",
    "\n",
    "Maximum granules per asynchronous request = 2000 \n",
    "\n",
    "We will set the access configuration depending on the number of granules requested. For requests over 2000 granules, we will produce multiple API endpoints for each 2000-granule order. Please note that synchronous requests may take a long time to complete depending on request parameters, so the number of granules may need to be adjusted if you are experiencing performance issues. The `page_size` parameter can be used to adjust this number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There will be 1 total order(s) processed for our SPL4SMGP request.\n"
     ]
    }
   ],
   "source": [
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "\n",
    "#Set the request mode to asynchronous if the number of granules is over 100, otherwise synchronous is enabled by default\n",
    "if len(granules) > 100:\n",
    "    request_mode = 'async'\n",
    "    page_size = 2000\n",
    "else: \n",
    "    page_size = 100\n",
    "    request_mode = 'stream'\n",
    "\n",
    "#Determine number of orders needed for requests over 2000 granules. \n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "\n",
    "print('There will be', page_num, 'total order(s) processed for our', short_name, 'request.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the API endpoint \n",
    "\n",
    "Programmatic API requests are formatted as HTTPS URLs that contain key-value-pairs specifying the service operations that we specified above. The following command can be executed via command line, a web browser, or in Python below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=SPL4SMGP&version=007&temporal=2018-07-01T00:00:00Z,2019-01-01T00:00:00Z&format=NetCDF4-CF&projection=GEOGRAPHIC&Coverage=/Geophysical_Data/precipitation_total_surface_flux,/cell_column,/cell_row,/cell_lat,/cell_lon&page_size=2000&request_mode=async&email=raraki8159@sdsu.edu&page_num=1\n"
     ]
    }
   ],
   "source": [
    "if aoi == '1':\n",
    "# bounding box search and subset:\n",
    "    param_dict = {'short_name': short_name, \n",
    "                  'version': latest_version, \n",
    "                  'temporal': temporal, \n",
    "                  'time': time_var, \n",
    "                  # 'bounding_box': bounding_box, \n",
    "                  # 'bbox': bbox, \n",
    "                  'format': reformat, \n",
    "                  'projection': projection, \n",
    "                  'projection_parameters': projection_parameters, \n",
    "                  'Coverage': coverage, \n",
    "                  'page_size': page_size, \n",
    "                  'request_mode': request_mode, \n",
    "                  'agent': agent, \n",
    "                  'email': email, }\n",
    "\n",
    "#Remove blank key-value-pairs\n",
    "param_dict = {k: v for k, v in param_dict.items() if v != ''}\n",
    "\n",
    "#Convert to string\n",
    "param_string = '&'.join(\"{!s}={!r}\".format(k,v) for (k,v) in param_dict.items())\n",
    "param_string = param_string.replace(\"'\",\"\")\n",
    "\n",
    "#Print API base URL + request parameters\n",
    "endpoint_list = [] \n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    API_request = api_request = f'{base_url}?{param_string}&page_num={page_val}'\n",
    "    endpoint_list.append(API_request)\n",
    "\n",
    "print(*endpoint_list, sep = \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now download data using the Python requests library. The data will be downloaded directly to this notebook directory in a new Outputs folder. The progress of each order will be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order:  1\n",
      "Request HTTP response:  201\n",
      "Order request URL:  https://n5eil02u.ecs.nsidc.org/egi/request?short_name=SPL4SMGP&version=007&temporal=2018-07-01T00%3A00%3A00Z%2C2019-01-01T00%3A00%3A00Z&format=NetCDF4-CF&projection=GEOGRAPHIC&Coverage=%2FGeophysical_Data%2Fprecipitation_total_surface_flux%2C%2Fcell_column%2C%2Fcell_row%2C%2Fcell_lat%2C%2Fcell_lon&page_size=2000&request_mode=async&email=raraki8159%40sdsu.edu\n",
      "order ID:  5000004009628\n",
      "status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000004009628\n",
      "HTTP response from order response URL:  201\n",
      "Data request  1  is submitting...\n",
      "Initial request status is  pending\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000004009628.zip?1\n",
      "Beginning download of zipped output...\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000004009628.zip?2\n",
      "Beginning download of zipped output...\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000004009628.zip?3\n",
      "Beginning download of zipped output...\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000004009628.zip?4\n",
      "Beginning download of zipped output...\n",
      "Data request 1 is complete.\n"
     ]
    }
   ],
   "source": [
    "# Create an output folder if the folder does not already exist.\n",
    "path = os.path.join(out_path, short_name + 'v1')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "# Different access methods depending on request mode:\n",
    "\n",
    "if request_mode=='async':\n",
    "    # Request data service for each page number, and unzip outputs\n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        print('Order: ', page_val)\n",
    "\n",
    "    # For all requests other than spatial file upload, use get function\n",
    "        request = session.get(base_url, params=param_dict)\n",
    "\n",
    "        print('Request HTTP response: ', request.status_code)\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "        request.raise_for_status()\n",
    "        print('Order request URL: ', request.url)\n",
    "        esir_root = ET.fromstring(request.content)\n",
    "        # print('Order request response XML content: ', request.content)\n",
    "\n",
    "    #Look up order ID\n",
    "        orderlist = []   \n",
    "        for order in esir_root.findall(\"./order/\"):\n",
    "            orderlist.append(order.text)\n",
    "        orderID = orderlist[0]\n",
    "        print('order ID: ', orderID)\n",
    "\n",
    "    #Create status URL\n",
    "        statusURL = base_url + '/' + orderID\n",
    "        print('status URL: ', statusURL)\n",
    "\n",
    "    #Find order status\n",
    "        request_response = session.get(statusURL)    \n",
    "        print('HTTP response from order response URL: ', request_response.status_code)\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "        request_response.raise_for_status()\n",
    "        request_root = ET.fromstring(request_response.content)\n",
    "        statuslist = []\n",
    "        for status in request_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Data request ', page_val, ' is submitting...')\n",
    "        print('Initial request status is ', status)\n",
    "\n",
    "    #Continue loop while request is still processing\n",
    "        while status == 'pending' or status == 'processing': \n",
    "            # print('Status is not complete. Trying again.')\n",
    "            time.sleep(60*5)\n",
    "            loop_response = session.get(statusURL)\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "            loop_response.raise_for_status()\n",
    "            loop_root = ET.fromstring(loop_response.content)\n",
    "\n",
    "    #find status\n",
    "            statuslist = []\n",
    "            for status in loop_root.findall(\"./requestStatus/\"):\n",
    "                statuslist.append(status.text)\n",
    "            status = statuslist[0]\n",
    "            # print('Retry request status is: ', status)\n",
    "            if status == 'pending' or status == 'processing':\n",
    "                continue\n",
    "\n",
    "    #Order can either complete, complete_with_errors, or fail:\n",
    "    # Provide complete_with_errors error message:\n",
    "        if status == 'complete_with_errors' or status == 'failed':\n",
    "            messagelist = []\n",
    "            for message in loop_root.findall(\"./processInfo/\"):\n",
    "                messagelist.append(message.text)\n",
    "            print('error messages:')\n",
    "            pprint.pprint(messagelist)\n",
    "\n",
    "    # Download zipped order if status is complete or complete_with_errors\n",
    "        if status == 'complete' or status == 'complete_with_errors':\n",
    "            for nzip in range(1,5):\n",
    "                try:\n",
    "                    downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/'  + orderID + '.zip?' +  str(nzip)\n",
    "                    print('Zip download URL: ', downloadURL)\n",
    "                    print('Beginning download of zipped output...')\n",
    "                    zip_response = session.get(downloadURL)\n",
    "                    # Raise bad request: Loop will stop for bad response code.\n",
    "                    zip_response.raise_for_status()\n",
    "                    with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "                        z.extractall(path)\n",
    "                except:\n",
    "                    continue\n",
    "            print('Data request', page_val, 'is complete.')\n",
    "        else: print('Request failed.')\n",
    "            \n",
    "else:\n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        print('Order: ', page_val)\n",
    "        print('Requesting...')\n",
    "        request = session.get(base_url, params=param_dict)\n",
    "        print('HTTP response from order response URL: ', request.status_code)\n",
    "        request.raise_for_status()\n",
    "        d = request.headers['content-disposition']\n",
    "        fname = re.findall('filename=(.+)', d)\n",
    "        dirname = os.path.join(path,fname[0].strip('\\\"'))\n",
    "        print('Downloading...')\n",
    "        open(dirname, 'wb').write(request.content)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "    \n",
    "    # Unzip outputs\n",
    "    for z in os.listdir(path): \n",
    "        if z.endswith('.zip'): \n",
    "            zip_name = path + \"/\" + z \n",
    "            zip_ref = zipfile.ZipFile(zip_name) \n",
    "            zip_ref.extractall(path) \n",
    "            zip_ref.close() \n",
    "            os.remove(zip_name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://n5eil02u.ecs.nsidc.org/esir/<order ID>.html/\n",
    "# 2016-2017\n",
    "# 5000003983740\n",
    "# 5000003983761\n",
    "# 5000003983789\n",
    "# 5000003983808\n",
    "# 5000003983849\n",
    "# 5000003983877\n",
    "# 5000003983895\n",
    "# 5000003983919\n",
    "# 5000003983937\n",
    "\n",
    "\n",
    "# 2017-2018\n",
    "# 5000003985742\n",
    "# 5000003985793\n",
    "# 5000003985831\n",
    "# 5000003985901\n",
    "# 5000003985864\n",
    "# 5000003985933\n",
    "# 5000003985965\n",
    "# 5000003986003\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we will clean up the Output folder by removing individual order folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip outputs\n",
    "\n",
    "for z in os.listdir(path): \n",
    "    if z.endswith('.zip'): \n",
    "        zip_name = path + \"/\" + z \n",
    "        zip_ref = zipfile.ZipFile(zip_name) \n",
    "        zip_ref.extractall(path) \n",
    "        zip_ref.close() \n",
    "        os.remove(zip_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Outputs folder by removing individual granule folders \n",
    "path = r'C:\\Users\\raraki8159\\Downloads'\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            shutil.move(os.path.join(root, file), path)\n",
    "        except OSError:\n",
    "            pass\n",
    "    for name in dirs:\n",
    "       if os.path.isdir(os.path.join(root, name)):\n",
    "            try:\n",
    "                os.rmdir(os.path.join(root, name))    \n",
    "            except OSError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To review, we have explored data availability and volume over a region and time of interest, discovered and selected data customization options, constructed an API endpoint for our request, and downloaded data directly to our local machine. You are welcome to request different data sets, areas of interest, and/or customization services by re-running the notebook or starting again at the 'Select a data set of interest' step above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing dates:\n",
      "2021-10-01 - 8 files missing\n",
      "2021-10-02 - 8 files missing\n",
      "2021-10-03 - 8 files missing\n",
      "2021-10-04 - 8 files missing\n",
      "2021-10-05 - 8 files missing\n",
      "2021-10-06 - 8 files missing\n",
      "2021-10-07 - 8 files missing\n",
      "2021-10-08 - 8 files missing\n",
      "2021-10-09 - 8 files missing\n",
      "2021-10-10 - 8 files missing\n",
      "2021-10-11 - 8 files missing\n",
      "2021-10-12 - 8 files missing\n",
      "2021-10-13 - 8 files missing\n",
      "2021-10-14 - 8 files missing\n",
      "2021-10-15 - 8 files missing\n",
      "2021-10-16 - 8 files missing\n",
      "2021-10-17 - 8 files missing\n",
      "2021-10-18 - 8 files missing\n",
      "2021-10-19 - 8 files missing\n",
      "2021-10-20 - 8 files missing\n",
      "2021-10-21 - 8 files missing\n",
      "2021-10-22 - 8 files missing\n",
      "2021-10-23 - 8 files missing\n",
      "2021-10-24 - 8 files missing\n",
      "2021-10-25 - 8 files missing\n",
      "2021-10-26 - 8 files missing\n",
      "2021-10-27 - 8 files missing\n",
      "2021-10-28 - 8 files missing\n",
      "2021-10-29 - 8 files missing\n",
      "2021-10-30 - 8 files missing\n",
      "2021-10-31 - 8 files missing\n",
      "2021-11-01 - 8 files missing\n",
      "2021-11-02 - 8 files missing\n",
      "2021-11-03 - 8 files missing\n",
      "2021-11-04 - 8 files missing\n",
      "2021-11-05 - 8 files missing\n",
      "2021-11-06 - 8 files missing\n",
      "2021-11-07 - 8 files missing\n",
      "2021-11-08 - 8 files missing\n",
      "2021-11-09 - 8 files missing\n",
      "2021-11-10 - 8 files missing\n",
      "2021-11-11 - 8 files missing\n",
      "2021-11-12 - 8 files missing\n",
      "2021-11-13 - 8 files missing\n",
      "2021-11-14 - 8 files missing\n",
      "2021-11-15 - 8 files missing\n",
      "2021-11-16 - 8 files missing\n",
      "2021-11-17 - 8 files missing\n",
      "2021-11-18 - 8 files missing\n",
      "2021-11-19 - 8 files missing\n",
      "2021-11-20 - 8 files missing\n",
      "2021-11-21 - 8 files missing\n",
      "2021-11-22 - 8 files missing\n",
      "2021-11-23 - 8 files missing\n",
      "2021-11-24 - 8 files missing\n",
      "2021-11-25 - 8 files missing\n",
      "2021-11-26 - 8 files missing\n",
      "2021-11-27 - 8 files missing\n",
      "2021-11-28 - 8 files missing\n",
      "2021-11-29 - 8 files missing\n",
      "2021-11-30 - 8 files missing\n",
      "2021-12-01 - 8 files missing\n",
      "2021-12-02 - 8 files missing\n",
      "2021-12-03 - 8 files missing\n",
      "2021-12-04 - 8 files missing\n",
      "2021-12-05 - 8 files missing\n",
      "2021-12-06 - 8 files missing\n",
      "2021-12-07 - 8 files missing\n",
      "2021-12-08 - 8 files missing\n",
      "2021-12-09 - 8 files missing\n",
      "2021-12-10 - 8 files missing\n",
      "2021-12-11 - 8 files missing\n",
      "2021-12-12 - 8 files missing\n",
      "2021-12-13 - 8 files missing\n",
      "2021-12-14 - 8 files missing\n",
      "2021-12-15 - 8 files missing\n",
      "2021-12-16 - 8 files missing\n",
      "2021-12-17 - 8 files missing\n",
      "2021-12-18 - 8 files missing\n",
      "2021-12-19 - 8 files missing\n",
      "2021-12-20 - 8 files missing\n",
      "2021-12-21 - 8 files missing\n",
      "2021-12-22 - 8 files missing\n",
      "2021-12-23 - 8 files missing\n",
      "2021-12-24 - 8 files missing\n",
      "2021-12-25 - 8 files missing\n",
      "2021-12-26 - 8 files missing\n",
      "2021-12-27 - 8 files missing\n",
      "2021-12-28 - 8 files missing\n",
      "2021-12-29 - 8 files missing\n",
      "2021-12-30 - 8 files missing\n",
      "2021-12-31 - 8 files missing\n",
      "2022-01-01 - 8 files missing\n",
      "2022-01-02 - 8 files missing\n",
      "2022-01-03 - 8 files missing\n",
      "2022-01-04 - 8 files missing\n",
      "2022-01-05 - 8 files missing\n",
      "2022-01-06 - 8 files missing\n",
      "2022-01-07 - 8 files missing\n",
      "2022-01-08 - 8 files missing\n",
      "2022-01-09 - 8 files missing\n",
      "2022-01-10 - 8 files missing\n",
      "2022-01-11 - 8 files missing\n",
      "2022-01-12 - 8 files missing\n",
      "2022-01-13 - 8 files missing\n",
      "2022-01-14 - 8 files missing\n",
      "2022-01-15 - 8 files missing\n",
      "2022-01-16 - 8 files missing\n",
      "2022-01-17 - 8 files missing\n",
      "2022-01-18 - 8 files missing\n",
      "2022-01-19 - 8 files missing\n",
      "2022-01-20 - 8 files missing\n",
      "2022-01-21 - 8 files missing\n",
      "2022-01-22 - 8 files missing\n",
      "2022-01-23 - 8 files missing\n",
      "2022-01-24 - 8 files missing\n",
      "2022-01-25 - 8 files missing\n",
      "2022-01-26 - 8 files missing\n",
      "2022-01-27 - 8 files missing\n",
      "2022-01-28 - 8 files missing\n",
      "2022-01-29 - 8 files missing\n",
      "2022-01-30 - 8 files missing\n",
      "2022-01-31 - 8 files missing\n",
      "2022-02-01 - 8 files missing\n",
      "2022-02-02 - 8 files missing\n",
      "2022-02-03 - 8 files missing\n",
      "2022-02-04 - 8 files missing\n",
      "2022-02-05 - 8 files missing\n",
      "2022-02-06 - 8 files missing\n",
      "2022-02-07 - 8 files missing\n",
      "2022-02-08 - 8 files missing\n",
      "2022-02-09 - 8 files missing\n",
      "2022-02-10 - 8 files missing\n",
      "2022-02-11 - 8 files missing\n",
      "2022-02-12 - 8 files missing\n",
      "2022-02-13 - 8 files missing\n",
      "2022-02-14 - 8 files missing\n",
      "2022-02-15 - 8 files missing\n",
      "2022-02-16 - 8 files missing\n",
      "2022-02-17 - 8 files missing\n",
      "2022-02-18 - 8 files missing\n",
      "2022-02-19 - 8 files missing\n",
      "2022-02-20 - 8 files missing\n",
      "2022-02-21 - 8 files missing\n",
      "2022-02-22 - 8 files missing\n",
      "2022-02-23 - 8 files missing\n",
      "2022-02-24 - 8 files missing\n",
      "2022-02-25 - 8 files missing\n",
      "2022-02-26 - 8 files missing\n",
      "2022-02-27 - 8 files missing\n",
      "2022-02-28 - 8 files missing\n",
      "2022-03-01 - 8 files missing\n",
      "2022-03-02 - 8 files missing\n",
      "2022-03-03 - 8 files missing\n",
      "2022-03-04 - 8 files missing\n",
      "2022-03-05 - 8 files missing\n",
      "2022-03-06 - 8 files missing\n",
      "2022-03-07 - 8 files missing\n",
      "2022-03-08 - 8 files missing\n",
      "2022-03-09 - 8 files missing\n",
      "2022-03-10 - 8 files missing\n",
      "2022-03-11 - 8 files missing\n",
      "2022-03-12 - 8 files missing\n",
      "2022-03-13 - 8 files missing\n",
      "2022-03-14 - 8 files missing\n",
      "2022-03-15 - 8 files missing\n",
      "2022-03-16 - 8 files missing\n",
      "2022-03-17 - 8 files missing\n",
      "2022-03-18 - 8 files missing\n",
      "2022-03-19 - 8 files missing\n",
      "2022-03-20 - 8 files missing\n",
      "2022-03-21 - 8 files missing\n",
      "2022-03-22 - 8 files missing\n",
      "2022-03-23 - 8 files missing\n",
      "2022-03-24 - 8 files missing\n",
      "2022-03-25 - 8 files missing\n",
      "2022-03-26 - 8 files missing\n",
      "2022-03-27 - 8 files missing\n",
      "2022-03-28 - 8 files missing\n",
      "2022-03-29 - 8 files missing\n",
      "2022-03-30 - 8 files missing\n",
      "Daily data is not complete from March 31, 2015 to March 30, 2022 with 8 files per day.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "def is_complete(folder):\n",
    "    # Create a dictionary to store the number of files seen for each date\n",
    "    seen_dates = {}\n",
    "    \n",
    "    # Loop through the files in the folder\n",
    "    for file in os.listdir(folder):\n",
    "        # Check if the file name matches the pattern\n",
    "        # Vv7032 is v7\n",
    "        # Vv7030 is v6\n",
    "        if not file.startswith(\"SMAP_L4_SM_gph_\") or not file.endswith(\"Vv7032_001_HEGOUT.nc\"):\n",
    "            continue\n",
    "        \n",
    "        # Extract the date from the file name\n",
    "        date_str = file[15:23]\n",
    "        # print(date_str)\n",
    "        # print(date_str)\n",
    "        try:\n",
    "            date = datetime.datetime.strptime(date_str, \"%Y%m%d\").date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # Add the date to the dictionary of seen dates only if it falls within the specified range\n",
    "        if date >= datetime.date(2015, 3, 31) and date <= datetime.date(2022, 3, 30):\n",
    "            if date in seen_dates:\n",
    "                seen_dates[date] += 1\n",
    "            else:\n",
    "                seen_dates[date] = 1\n",
    "        \n",
    "    \n",
    "    # Create a set of all the dates from March 31, 2015 to March 30, 2022\n",
    "    all_dates = set()\n",
    "    start_date = datetime.date(2015, 3, 31)\n",
    "    end_date = datetime.date(2022, 3, 30)\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        all_dates.add(current_date)\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "    # Check if there are 8 files per day\n",
    "    missing_dates = set()\n",
    "    for date in all_dates:\n",
    "        if date not in seen_dates or seen_dates[date] != 8:\n",
    "            missing_dates.add(date)\n",
    "\n",
    "    if missing_dates:\n",
    "        print(\"Missing dates:\")\n",
    "        for date in sorted(missing_dates):\n",
    "            print(f\"{date} - {8 - seen_dates.get(date,0)} files missing\")\n",
    "    else:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "folder = r'G:\\Araki\\SMSigxSMAP\\1_data\\SPL4SMGP'\n",
    "if is_complete(folder):\n",
    "    print(\"Daily data is complete from March 31, 2015 to March 30, 2022 with 8 files per day.\")\n",
    "else:\n",
    "    print(\"Daily data is not complete from March 31, 2015 to March 30, 2022 with 8 files per day.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
